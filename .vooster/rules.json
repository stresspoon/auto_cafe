{
  "rules": [
    {
      "type": "prd",
      "content": "# 제품 요구사항 문서 (PRD)\n\n## 1. 총괄 요약\n네이버 카페에 업로드된 챌린지 미션 현황을 매일 00시 자동 확인하여 구글 시트에 `O/X`로 표기하는 자동화 서비스. 챌린지 진행자는 수작업 없이 참여자 진행 상황을 실시간 파악할 수 있어 운영 효율이 향상된다.\n\n## 2. 문제 정의\n- 진행자는 매일 카페 게시글을 열람하며 참여자별 업로드 여부를 수동 확인하고 시트에 기록함.  \n- 인원·주차 증가 시 확인 누락, 입력 오류, 시간 소모 발생.  \n- 현행 프로세스는 반복적·비효율적이며 자동화 솔루션 부재.\n\n## 3. 목표 및 목적\n- 1차 목표: 네이버 카페 → 구글 시트 자동 동기화\n- 부가 목표: 운영 시간 90% 절감, 입력 오류 0건 달성, 참여 현황 실시간 공개\n- 성공 지표:\n  - 매일 00:00 자동 실행 성공률 99% 이상\n  - 시트 반영 정확도 99.9% 이상\n  - 운영자 주간 업무 시간 5시간 → 0.5시간 축소\n\n## 4. 타깃 사용자\n### 주요 사용자\n- 챌린지 진행자: 20~40대, 온라인 강의·챌린지 운영 경험, 자동화 니즈 높음\n### 2차 사용자\n- 참여자: 개인 진척 상황 확인 가능  \n- 경영진: 활동 데이터 기반 리포트 활용\n\n## 5. 사용자 스토리\n- 진행자로서, 매일 수동 확인 없이도 시트에 자동 체크되어 운영 시간을 절약하고 싶다.  \n- 진행자로서, 참여 주차별 현황을 한눈에 보고 미달자를 즉시 파악하고 싶다.  \n- 참여자로서, 내 미션 업로드가 제대로 기록되었는지 즉시 확인하고 싶다.\n\n## 6. 기능 요구사항\n### 핵심 기능\n1. 네이버 자동 로그인  \n   - 입력된 ID/PW, 2차 인증 처리(OTP API 또는 Cookie 저장)  \n   - AC: 로그인 실패 시 재시도 3회, 실패 로깅\n\n2. 카페 게시글 크롤링  \n   - 지정 게시판 1~3페이지 순회, 제목·작성자·본문 파싱  \n   - `*주차` 키워드와 작성자 닉네임 추출  \n   - AC: 100건/page, 중복·삭제 게시글 제외\n\n3. 구글 시트 업데이트  \n   - 사전 정의된 Sheet ID, 사용자명 행, 주차 열에 `O` 기록, 없으면 `X`  \n   - AC: API 호출 오류 시 재시도, 최대 5초 내 응답\n\n4. 스케줄러  \n   - 매일 00:00 UTC+9 자동 실행  \n   - AC: Cron 기반, 실행 로그 저장, 실패 시 이메일 알림\n\n### 보조 기능\n- 수동 즉시 실행 버튼 (웹 UI 또는 CLI)  \n- 최근 실행 로그 조회  \n- 설정 파일(카페 URL, 시트 ID, 주차 수) UI 편집\n\n## 7. 비기능 요구사항\n- 성능: 1~3페이지 크롤링 및 시트 업데이트 2분 이내 완료  \n- 보안: 자격 증명 암호화 저장(AES256), HTTPS 통신  \n- 사용성: 설정 UI 국문, 3-step 이내 설정 완료  \n- 확장성: 주차·페이지 수 증가 시 5만 게시글까지 대응  \n- 호환성: Windows/Linux 서버, Python 3.10 이상\n\n## 8. 기술 고려사항\n- 아키텍처: \n  - `Scheduler` → `Crawler(Naver)` → `Parser` → `GSuite API Service` → `Logger`\n- 스택: Python, Selenium/Playwright, Google Sheets API v4, Cron, Docker  \n- 데이터: 게시글 메타(작성자, 작성일, 주차), 시트 셀 값  \n- 외부 연동: Google OAuth, 네이버 로그인, SendGrid(알림)  \n- 배포: Docker 이미지 → AWS ECS 또는 EC2\n\n## 9. 성공 지표\n- 일일 Task 성공률  \n- 시트 반영 시간(평균, p95)  \n- 수동 수정 건수  \n- 운영자 만족도 설문(4점/5점 이상)\n\n## 10. 일정 및 마일스톤\n- Phase 0 (2주): 요구사항 확정, 기술 검증(PoC)  \n- Phase 1 (4주): MVP  \n  - 자동 로그인, 1페이지 크롤링, 시트 반영  \n- Phase 2 (3주): 범위 확대  \n  - 3페이지, 주차 자동 인식, 알림  \n- Phase 3 (2주): UI·모니터링, 클라우드 배포  \n- 런칭: 2024-Q3 Week2\n\n## 11. 위험 및 대응\n- 네이버 보안 정책 변경 → 크롤링 차단  \n  - 대응: Selenium 스텔스 모드, 쿠키 재생성 스크립트  \n- Google API 호출 제한  \n  - 대응: 배치 쓰로틀링, 캐시  \n- 00시 트래픽 집중 → 실행 지연  \n  - 대응: 재시도 로직, 오프피크 시간 옵션  \n- 사용자 인증정보 유출  \n  - 대응: KeyVault 저장, 정기 교체\n\n## 12. 향후 고려사항\n- 멀티 카페·게시판 지원  \n- Slack/카카오 알림 통합  \n- 대시보드 시각화(Google Data Studio)  \n- AI를 활용한 게시글 자동 분류 및 부정행위 탐지",
      "writedAt": "2025-07-22T12:36:42.167Z"
    },
    {
      "type": "architecture",
      "content": "# 기술 요구사항 문서 (TRD)\n\n## 1. 기술 총괄 요약\n### 프로젝트 개요\n본 프로젝트는 네이버 카페의 챌린지 미션 현황을 매일 00시에 자동으로 확인하여 구글 시트에 `O/X`로 표기하는 자동화 서비스를 구축하는 것을 목표로 합니다. 핵심은 수동 작업을 최소화하고, 운영 효율성을 극대화하는 데 있습니다. 이를 위해 Python 기반의 스크립트와 웹 자동화 도구를 활용하여 네이버 카페에 접근하고, 크롤링된 데이터를 Google Sheets API를 통해 구글 시트에 반영하는 아키텍처를 채택합니다.\n\n### 핵심 기술 스택\nPython, Playwright, Google Sheets API v4, Cron, Docker\n\n### 주요 기술 목표\n- **성능**: 1~3페이지 크롤링 및 시트 업데이트 2분 이내 완료\n- **정확성**: 시트 반영 정확도 99.9% 이상\n- **안정성**: 매일 00:00 자동 실행 성공률 99% 이상\n- **보안**: 자격 증명 암호화 저장 및 HTTPS 통신\n- **확장성**: 주차 및 페이지 수 증가 시 5만 게시글까지 대응 가능하도록 설계\n\n### 주요 기술 가정\n- 네이버 카페의 HTML 구조는 급격하게 변경되지 않으며, 변경 시 대응 가능한 수준으로 가정합니다.\n- Google Sheets API의 호출 제한은 현재 요구사항을 충족하는 범위 내에서 발생하지 않거나, 발생 시 적절한 재시도 및 지연 전략으로 극복 가능하다고 가정합니다.\n- 2차 인증(OTP) 처리는 Playwright를 통한 자동화 또는 쿠키 저장을 통해 우회 가능하다고 가정합니다.\n\n## 2. 기술 스택\n\n| Category | Technology / Library | Reasoning (Why it's chosen for this project) |\n| :---------------- | :--------------------------- | :------------------------------------------- |\n| **주요 언어** | Python | 빠른 개발 속도, 다양한 라이브러리 지원(웹 크롤링, API 연동), 스크립트 자동화에 적합 |\n| **웹 자동화** | Playwright | Selenium 대비 경량화, 비동기 지원, 다양한 브라우저 지원, 스텔스 모드 구현 용이, 네이버 로그인 및 크롤링에 최적 |\n| **Google API 연동** | Google Sheets API v4 | 구글 시트 데이터 읽기/쓰기 공식 API, 안정적이고 강력한 기능 제공 |\n| **스케줄링** | Cron | Linux/Unix 기반 시스템에서 가장 보편적이고 안정적인 스케줄러, Docker 환경에서 쉽게 통합 가능 |\n| **환경 격리/배포** | Docker | 개발/운영 환경 일관성 유지, 의존성 관리 용이, 배포 및 확장에 유리 |\n| **로깅** | `logging` (Python 표준 라이브러리) | 별도 라이브러리 없이 기본적인 로깅 기능 제공, 경량화 및 유지보수 용이 |\n| **설정 관리** | `configparser` 또는 `json` (Python 표준 라이브러리) | 간단한 설정 파일 관리에 적합, 외부 라이브러리 의존성 없음 |\n| **보안 (자격 증명)** | 환경 변수 / AWS Secrets Manager (향후) | 민감 정보의 코드 내 하드코딩 방지, 초기에는 환경 변수 사용, 확장 시 전문 솔루션 고려 |\n| **알림** | `smtplib` (Python 표준 라이브러리) | Cron 실패 시 이메일 알림을 위한 간단한 SMTP 클라이언트 기능 제공 |\n\n## 3. 시스템 아키텍처 설계\n\n### 최상위 빌딩 블록\n- **스케줄러 (Scheduler)**: 매일 00:00 (UTC+9)에 전체 자동화 프로세스를 트리거하는 역할을 합니다. Cron 기반으로 동작하며, 실행 로그를 기록하고 실패 시 알림을 보냅니다.\n- **크롤러 (Crawler)**: 네이버 카페에 자동 로그인하고, 지정된 게시판의 게시글을 순회하며 필요한 데이터를 추출합니다. Playwright를 사용하여 웹 자동화를 수행합니다.\n- **파서 (Parser)**: 크롤러가 수집한 게시글 데이터(제목, 작성자, 본문)에서 `*주차` 키워드와 작성자 닉네임을 정규식을 활용하여 추출하고 정제합니다.\n- **Google Sheets API 서비스 (GSheets API Service)**: 파싱된 데이터를 기반으로 구글 시트에 접속하여, 사전 정의된 Sheet ID, 사용자명 행, 주차 열에 `O` 또는 `X`를 기록합니다. Google Sheets API v4를 사용합니다.\n- **로거 (Logger)**: 시스템의 모든 주요 동작(로그인 시도, 크롤링 결과, 시트 업데이트 결과, 오류 등)을 기록합니다.\n\n### 최상위 컴포넌트 상호작용 다이어그램\n\n```mermaid\ngraph TD\n    A[스케줄러] --> B[크롤러]\n    B --> C[파서]\n    C --> D[Google Sheets API 서비스]\n    D --> E[구글 시트]\n    B --> F[네이버 카페]\n    D --> G[로거]\n    A --> G\n    A -- 실패 시 --> H[이메일 알림]\n```\n\n- **스케줄러**는 정해진 시간에 **크롤러**를 실행하여 네이버 카페 자동화 프로세스를 시작합니다.\n- **크롤러**는 **네이버 카페**에 접속하여 게시글 데이터를 수집하고, 수집된 데이터를 **파서**에게 전달합니다.\n- **파서**는 전달받은 데이터를 가공하여 **Google Sheets API 서비스**가 처리할 수 있는 형태로 변환합니다.\n- **Google Sheets API 서비스**는 가공된 데이터를 기반으로 **구글 시트**에 접근하여 데이터를 업데이트하고, 모든 작업 과정은 **로거**에 기록됩니다.\n- **스케줄러**는 실행 결과 및 오류 발생 시 **로거**에 기록하고, 중요한 실패의 경우 **이메일 알림**을 발송합니다.\n\n### 코드 조직화 및 컨벤션\n**도메인 중심 조직화 전략**\n- **도메인 분리**: 비즈니스 도메인별로 코드를 구성합니다. (예: `naver_crawler`, `google_sheets`, `scheduler`, `core`)\n- **계층 기반 아키텍처**: 각 도메인 내에서 관심사를 분리하여 계층을 나눕니다. (예: `services`, `utils`, `models`)\n- **기능 기반 모듈**: 특정 기능과 관련된 모든 요소를 하나의 모듈 또는 디렉토리에 함께 배치합니다.\n- **공유 컴포넌트**: 공통 유틸리티, 타입 정의, 재사용 가능한 컴포넌트는 별도의 `shared` 또는 `common` 모듈에 위치시킵니다.\n\n**범용 파일 및 폴더 구조**\n```\n/\n├── src/\n│   ├── main.py                     # 메인 실행 스크립트 (스케줄러 진입점)\n│   ├── config.py                   # 설정 로드 및 관리\n│   ├── core/\n│   │   ├── exceptions.py           # 사용자 정의 예외\n│   │   ├── logger.py               # 로깅 설정 및 유틸리티\n│   │   └── security.py             # 자격 증명 암호화/복호화\n│   ├── naver_crawler/\n│   │   ├── __init__.py\n│   │   ├── service.py              # 네이버 카페 크롤링 및 로그인 로직\n│   │   └── models.py               # 네이버 게시글 관련 데이터 모델\n│   ├── google_sheets/\n│   │   ├── __init__.py\n│   │   ├── service.py              # 구글 시트 연동 로직\n│   │   └── models.py               # 구글 시트 관련 데이터 모델\n│   ├── parser/\n│   │   ├── __init__.py\n│   │   └── service.py              # 게시글 데이터 파싱 로직\n│   ├── scheduler/\n│   │   ├── __init__.py\n│   │   └── service.py              # 스케줄링 및 알림 로직\n│   └── shared/\n│       ├── __init__.py\n│       └── utils.py                # 공통 유틸리티 함수\n├── tests/                          # 테스트 코드\n│   ├── unit/\n│   ├── integration/\n├── config/\n│   └── settings.ini               # 설정 파일 (민감 정보 제외)\n├── data/\n│   └── credentials.json            # Google API 서비스 계정 키 (환경 변수 또는 Secrets Manager 권장)\n├── .env                            # 환경 변수 (민감 정보)\n├── Dockerfile                      # Docker 이미지 빌드 파일\n├── requirements.txt                # Python 의존성 목록\n├── README.md                       # 프로젝트 설명\n```\n\n### 데이터 흐름 및 통신 패턴\n- **클라이언트-서버 통신 (내부)**:\n    - 스케줄러는 내부적으로 크롤러, 파서, Google Sheets API 서비스 모듈의 함수를 호출하는 방식으로 통신합니다.\n    - 각 서비스 모듈은 필요한 데이터를 함수 인자로 전달받고, 처리 결과를 반환합니다.\n- **클라이언트-서버 통신 (외부)**:\n    - **네이버 카페**: Playwright를 통해 HTTP/HTTPS 프로토콜로 네이버 서버와 통신합니다. 로그인 요청, 게시판 페이지 요청, 게시글 본문 요청 등이 포함됩니다.\n    - **Google Sheets API**: HTTPS 프로토콜을 통해 Google API 서버와 통신합니다. OAuth 2.0 또는 서비스 계정 인증을 사용하여 시트 읽기/쓰기 요청을 보냅니다.\n- **데이터베이스 상호작용**:\n    - 본 프로젝트는 별도의 영구 데이터베이스를 사용하지 않습니다. 모든 데이터는 크롤링 시점에 메모리에서 처리되고, 최종 결과는 구글 시트에 반영됩니다.\n- **외부 서비스 통합**:\n    - **Google OAuth**: Google Sheets API 접근을 위한 인증 메커니즘으로 사용됩니다. 서비스 계정 또는 사용자 인증 정보(최초 1회 인증 후 토큰 저장)를 활용합니다.\n    - **네이버 로그인**: Playwright를 통해 웹 페이지에서 직접 ID/PW를 입력하고 2차 인증(OTP 또는 쿠키)을 처리하는 방식으로 진행됩니다.\n    - **이메일 알림**: Python의 `smtplib`를 사용하여 SMTP 서버를 통해 이메일을 발송합니다. (SendGrid와 같은 외부 서비스는 향후 확장 시 고려)\n- **실시간 통신**:\n    - 본 프로젝트에서는 실시간 통신(WebSocket, SSE)이 필요하지 않습니다. 모든 작업은 스케줄러에 의해 주기적으로 실행되는 배치 작업입니다.\n- **데이터 동기화**:\n    - 구글 시트가 최종적인 데이터 저장소 역할을 합니다. 크롤링 및 파싱된 데이터는 구글 시트에 직접 반영되어 동기화됩니다. 충돌 발생 시 Google Sheets API의 기본 동작(덮어쓰기)을 따르며, 재시도 로직으로 일관성을 유지합니다.\n\n## 4. 성능 및 최적화 전략\n\n- **병렬 처리 최소화**: 초기 MVP 단계에서는 복잡한 병렬 처리를 지양하고, 순차적인 실행을 통해 안정성을 확보합니다. 필요 시 Playwright의 비동기 기능을 활용하여 I/O 바운드 작업을 최적화합니다.\n- **선택적 크롤링**: 지정된 1~3페이지 내에서만 크롤링을 수행하여 불필요한 네트워크 요청을 줄입니다. 게시글 제목에 `*주차` 키워드가 없는 경우 본문 파싱을 건너뛰어 처리 시간을 단축합니다.\n- **Google Sheets API 배치 업데이트**: 개별 셀 업데이트 대신 `spreadsheets.values.batchUpdate`와 같은 배치 업데이트 기능을 사용하여 API 호출 횟수를 최소화하고 성능을 향상시킵니다.\n- **재시도 및 지연 전략**: 네트워크 오류, API 호출 제한 등에 대비하여 지수 백오프(Exponential Backoff)를 포함한 재시도 로직을 구현하여 안정성을 높입니다.\n- **리소스 최적화**: Docker 컨테이너의 메모리 및 CPU 사용량을 모니터링하고, 불필요한 의존성을 제거하여 경량화된 환경을 유지합니다. Playwright 브라우저 인스턴스는 작업 완료 후 반드시 종료하여 리소스를 해제합니다.\n\n## 5. 구현 로드맵 및 마일스톤\n\n### Phase 1: 기반 구축 (MVP 구현)\n- **핵심 인프라**: Python 개발 환경, Dockerfile 구성, 기본 프로젝트 구조 설정\n- **필수 기능**:\n    - 네이버 자동 로그인 (ID/PW, 2차 인증 처리 - Playwright 활용)\n    - 지정 게시판 1페이지 크롤링 (제목, 작성자, 본문 파싱)\n    - 구글 시트 업데이트 (사전 정의된 Sheet ID, 사용자명 행, 주차 열에 `O` 기록)\n- **기본 보안**: 자격 증명 환경 변수 또는 설정 파일 분리\n- **개발 환경 설정**: 로컬 개발 환경 구성, `requirements.txt` 관리\n- **예상 기간**: 4주\n\n### Phase 2: 기능 확장\n- **고급 기능**:\n    - 카페 게시글 크롤링 범위 확장 (1~3페이지 순회)\n    - `*주차` 키워드와 작성자 닉네임 추출 로직 고도화\n    - 구글 시트 업데이트 로직 개선 (`O` 기록, 없으면 `X` 기록)\n    - 스케줄러 구현 (Cron 기반, 매일 00:00 UTC+9 자동 실행)\n    - 실행 로그 저장 기능 구현\n    - 실패 시 이메일 알림 기능 구현\n- **성능 최적화**: Google Sheets API 배치 업데이트 적용\n- **향상된 보안**: 자격 증명 암호화 저장 (AES256)\n- **예상 기간**: 3주\n\n### Phase 3: 확장 및 최적화\n- **확장성 구현**: 5만 게시글 대응을 위한 크롤링 및 파싱 로직 최적화 (메모리 효율성)\n- **고급 통합**: (선택 사항) AWS Secrets Manager 연동을 통한 자격 증명 관리\n- **모니터링 구현**: 실행 성공률, 시트 반영 시간 등 주요 지표 로깅 및 모니터링 준비\n- **배포**: Docker 이미지 빌드 및 AWS ECS 또는 EC2 배포 스크립트/가이드 작성\n- **예상 기간**: 2주\n\n## 6. 위험 평가 및 완화 전략\n\n### 기술 위험 분석\n- **기술 위험**: 네이버 보안 정책 변경 → 크롤링 차단\n    - **완화 전략**: Playwright의 스텔스 모드 및 헤더 조작을 통해 봇 탐지 회피 시도. 쿠키 기반 로그인 유지 스크립트 구현. 주기적인 모니터링 및 변경 감지 시 빠른 코드 업데이트.\n- **성능 위험**: 00시 트래픽 집중 → 실행 지연\n    - **완화 전략**: Playwright의 `headless` 모드 사용으로 리소스 절약. Google Sheets API 배치 업데이트 적용. 크롤링 및 시트 업데이트 로직 최적화. 재시도 로직에 지연 시간 추가.\n- **보안 위험**: 사용자 인증정보 유출\n    - **완화 전략**: 네이버 ID/PW 및 Google API 키는 환경 변수 또는 암호화된 파일로 저장. 코드 내 하드코딩 금지. AES256 암호화 적용. HTTPS 통신 강제. (향후 AWS Secrets Manager와 같은 전문 솔루션 도입 고려)\n- **통합 위험**: Google API 호출 제한\n    - **완화 전략**: Google Sheets API의 할당량 및 사용량 모니터링. 배치 업데이트를 통한 API 호출 횟수 최소화. API 호출 실패 시 지수 백오프를 적용한 재시도 로직 구현.\n\n### 프로젝트 전달 위험\n- **일정 위험**: 개발 일정 지연\n    - **완화 전략**: 각 Phase별 명확한 목표 설정 및 주간 진행 상황 공유. PoC를 통해 초기 기술 위험을 최소화. 핵심 기능 우선 개발 (MVP).\n- **자원 위험**: 개발 인력 부족 또는 기술 전문성 부족\n    - **완화 전략**: Python, Playwright, Docker 등 핵심 기술에 대한 팀원 교육 및 스터디 진행. 필요한 경우 외부 전문가 자문 고려.\n- **품질 위험**: 코드 품질 저하 및 버그 발생\n    - **완화 전략**: 코드 리뷰 프로세스 도입. 단위 테스트 및 통합 테스트 작성. CI/CD 파이프라인 구축을 통한 자동화된 테스트 실행.\n- **배포 위험**: 프로덕션 환경 배포 문제\n    - **완화 전략**: Docker를 통한 개발/운영 환경 일관성 유지. 단계별 배포 전략 (개발 → 스테이징 → 프로덕션). 롤백 계획 수립.\n- **비상 계획**:\n    - 자동화 시스템 장애 시, 수동으로 네이버 카페 확인 및 구글 시트 업데이트를 수행할 수 있는 비상 절차 마련.\n    - 주요 오류 발생 시 개발팀에 즉시 알림이 가도록 모니터링 및 알림 시스템 구축.",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "guideline",
      "content": "# Project Code Guidelines\n\n## 1. Project Overview\n\nThis document outlines the coding standards and best practices for the automated challenge mission status tracking service. The service automatically verifies challenge mission completion from Naver Cafe and updates Google Sheets daily. Key architectural decisions involve using Python for scripting, Playwright for web automation, Google Sheets API v4 for data synchronization, Cron for scheduling, and Docker for environment consistency and deployment. The system is designed for high accuracy, stability, and scalability, handling up to 50,000 posts.\n\n## 2. Core Principles\n\n*   **Readability & Maintainability**: Prioritize clear, self-documenting code that is easy to understand and modify by any team member.\n*   **Modularity & Separation of Concerns**: Design components with single responsibilities, minimizing coupling and maximizing reusability.\n*   **Robustness & Error Handling**: Implement comprehensive error handling, retry mechanisms, and logging to ensure system resilience and operational visibility.\n*   **Security & Data Integrity**: Safeguard sensitive information through encryption and secure handling, ensuring data accuracy in Google Sheets.\n*   **Performance & Efficiency**: Optimize critical paths to meet the 2-minute processing time for crawling and sheet updates, avoiding unnecessary resource consumption.\n\n## 3. Language-Specific Guidelines (Python)\n\n### File Organization and Directory Structure\n\nThe project adheres to a domain-centric organization strategy, placing related functionalities within dedicated modules.\n\n*   **MUST**: Organize code into logical domains (e.g., `naver_crawler`, `google_sheets`, `parser`, `scheduler`) under the `src/` directory.\n*   **MUST**: Place shared utilities, exceptions, and logging configuration in `src/core/` and `src/shared/`.\n*   **MUST**: Use `__init__.py` files in all package directories to mark them as Python packages.\n\n```\nsrc/\n├── main.py\n├── config.py\n├── core/\n│   ├── exceptions.py\n│   ├── logger.py\n│   └── security.py\n├── naver_crawler/\n│   ├── __init__.py\n│   ├── service.py\n│   └── models.py\n└── ...\n```\n\n### Import/Dependency Management\n\n*   **MUST**: Use absolute imports for internal modules (e.g., `from src.naver_crawler import service`).\n*   **MUST**: Group imports in the following order, separated by a blank line:\n    1.  Standard library imports\n    2.  Third-party library imports\n    3.  Local application/library imports\n*   **MUST**: Use `requirements.txt` for managing all project dependencies.\n*   **MUST NOT**: Use relative imports (e.g., `from . import service`) within sub-packages, as this can lead to ambiguity and refactoring issues.\n\n```python\n# MUST: Correct import order and style\nimport logging\nimport os\n\nimport playwright\nfrom google.oauth2 import service_account\n\nfrom src.core.exceptions import CrawlerError\nfrom src.naver_crawler.models import NaverPost\n```\n\n```python\n# MUST NOT: Incorrect relative import or unorganized imports\nimport os\nfrom .models import NaverPost # Relative import\nimport logging\nfrom google.oauth2 import service_account\n```\n\n### Error Handling Patterns\n\n*   **MUST**: Define custom exceptions for domain-specific errors in `src/core/exceptions.py`.\n*   **MUST**: Catch specific exceptions rather than broad `Exception` where possible.\n*   **MUST**: Log errors with appropriate severity levels (e.g., `logging.error`, `logging.exception`).\n*   **MUST**: Implement retry logic with exponential backoff for transient errors (e.g., network issues, API rate limits).\n*   **MUST NOT**: Suppress exceptions silently or use bare `except:` clauses.\n\n```python\n# MUST: Specific exception handling with logging and retry\nimport logging\nimport time\n\nfrom src.core.exceptions import GoogleSheetsAPIError\n\nlogger = logging.getLogger(__name__)\n\ndef update_sheet_with_retry(data, max_retries=3):\n    for attempt in range(max_retries):\n        try:\n            # Simulate API call\n            if attempt < 1: # Simulate failure on first attempt\n                raise GoogleSheetsAPIError(\"API rate limit exceeded.\")\n            logger.info(\"Sheet updated successfully.\")\n            return True\n        except GoogleSheetsAPIError as e:\n            logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {2**attempt} seconds...\")\n            time.sleep(2**attempt)\n    logger.error(f\"Failed to update sheet after {max_retries} attempts.\")\n    return False\n```\n\n```python\n# MUST NOT: Broad exception handling or silent suppression\ndef update_sheet_bad(data):\n    try:\n        # Some API call\n        pass\n    except: # Catches all exceptions, hides specific issues\n        print(\"An error occurred.\") # No logging, no specific error handling\n```\n\n## 4. Code Style Rules\n\n### MUST Follow:\n\n*   **PEP 8 Compliance**: Adhere strictly to PEP 8 for naming conventions, line length (max 120 chars), whitespace, and overall code formatting.\n    *   **Rationale**: Ensures consistency and readability across the codebase.\n*   **Meaningful Naming**: Use descriptive names for variables, functions, classes, and modules.\n    *   **Rationale**: Improves code comprehension and reduces the need for extensive comments.\n    ```python\n    # MUST: Clear and descriptive names\n    def get_naver_posts_by_week(board_id: str, week_number: int) -> list[NaverPost]:\n        \"\"\"Fetches Naver posts for a specific week.\"\"\"\n        pass\n    ```\n    ```python\n    # MUST NOT: Ambiguous names\n    def get_data(id, num): # What data? What id? What num?\n        pass\n    ```\n*   **Type Hinting**: Use type hints for function arguments, return values, and variables.\n    *   **Rationale**: Enhances code readability, enables static analysis, and improves maintainability.\n    ```python\n    # MUST: Use type hints\n    def parse_post_content(content: str) -> tuple[int, str]:\n        \"\"\"Extracts week number and author from post content.\"\"\"\n        week_match = re.search(r'\\*(\\d+)주차', content)\n        author_match = re.search(r'작성자: (\\S+)', content)\n        week = int(week_match.group(1)) if week_match else 0\n        author = author_match.group(1) if author_match else \"Unknown\"\n        return week, author\n    ```\n    ```python\n    # MUST NOT: Lack of type hints\n    def parse_post_content(content): # Unclear input/output types\n        # ...\n        return week, author\n    ```\n*   **Docstrings**: Provide comprehensive docstrings for all modules, classes, methods, and functions following Google or NumPy style.\n    *   **Rationale**: Documents purpose, arguments, and return values, aiding understanding and API generation.\n*   **Logging**: Use the Python `logging` module for all application logs. Configure log levels appropriately.\n    *   **Rationale**: Centralized and configurable logging is crucial for debugging, monitoring, and auditing.\n    ```python\n    # MUST: Use the logging module\n    import logging\n    logger = logging.getLogger(__name__)\n\n    def process_challenge_status():\n        logger.info(\"Starting challenge status processing.\")\n        try:\n            # ...\n            logger.debug(\"Successfully processed challenge data.\")\n        except Exception as e:\n            logger.error(f\"Error during processing: {e}\", exc_info=True)\n    ```\n    ```python\n    # MUST NOT: Use print statements for logging\n    def process_challenge_status_bad():\n        print(\"Starting challenge status processing.\") # Not configurable, not structured\n        # ...\n        print(\"Successfully processed challenge data.\")\n    ```\n*   **Configuration Management**: Load configurations from `config/settings.ini` or environment variables, especially for sensitive data.\n    *   **Rationale**: Separates configuration from code, making the application more flexible and secure.\n    ```python\n    # MUST: Load sensitive info from environment variables\n    import os\n    NAVER_ID = os.getenv(\"NAVER_ID\")\n    NAVER_PW = os.getenv(\"NAVER_PW\")\n    ```\n    ```python\n    # MUST NOT: Hardcode sensitive information\n    NAVER_ID = \"my_naver_id\" # Security risk\n    NAVER_PW = \"my_naver_password\"\n    ```\n\n### MUST NOT Do:\n\n*   **Huge, Multi-Responsibility Modules**: Avoid creating single files or classes that handle multiple, unrelated responsibilities.\n    *   **Rationale**: Violates the Single Responsibility Principle (SRP), making code harder to test, maintain, and understand.\n*   **Complex State Management**: Avoid overly complex state machines or global mutable states that are difficult to track and debug.\n    *   **Rationale**: Increases complexity, potential for bugs, and makes concurrent operations challenging.\n*   **Magic Numbers/Strings**: Do not use hardcoded literal values without explanation. Define them as constants.\n    *   **Rationale**: Improves readability and maintainability; changes only need to be made in one place.\n    ```python\n    # MUST: Use named constants\n    MAX_RETRIES = 3\n    DEFAULT_TIMEOUT_SECONDS = 60\n    ```\n    ```python\n    # MUST NOT: Use magic numbers\n    time.sleep(5) # What does 5 mean?\n    if count > 100: # What does 100 represent?\n        pass\n    ```\n*   **Direct Browser Interaction in Core Logic**: Do not embed Playwright browser interactions directly within data parsing or sheet update logic.\n    *   **Rationale**: Maintains clear separation of concerns; the crawler service should handle browser interaction, passing raw data to the parser.\n\n## 5. Architecture Patterns\n\n### Component/Module Structure Guidelines\n\n*   **Service Layer**: Each top-level domain (e.g., `naver_crawler`, `google_sheets`, `parser`, `scheduler`) will have a `service.py` file containing the core business logic for that domain.\n    *   **Rationale**: Encapsulates domain-specific operations, making them reusable and testable.\n*   **Model Layer**: Data structures (e.g., `NaverPost`, `SheetRow`) will be defined in `models.py` within their respective domain packages. Use `dataclasses` or `pydantic` for structured data.\n    *   **Rationale**: Provides clear data contracts and facilitates data validation.\n*   **Utility Modules**: Common, generic functions that can be used across multiple domains will reside in `src/shared/utils.py`.\n    *   **Rationale**: Prevents code duplication and promotes reusability.\n\n### Data Flow Patterns\n\n*   **Unidirectional Data Flow**: Data should generally flow in one direction: Scheduler -> Crawler -> Parser -> Google Sheets API Service.\n    *   **Rationale**: Simplifies debugging and understanding of data transformations.\n*   **Input/Output Contracts**: Each major function or service method should clearly define its expected inputs and guaranteed outputs using type hints and docstrings.\n    *   **Rationale**: Enforces clear interfaces between components.\n\n### State Management Conventions\n\n*   **Stateless Services (where possible)**: Design services to be largely stateless, processing inputs and returning outputs without retaining internal state across multiple calls.\n    *   **Rationale**: Improves scalability, testability, and reduces side effects.\n*   **Limited State for Automation**: Playwright browser instances will manage their own session state (cookies, local storage) within the `naver_crawler.service`. This state should be explicitly managed (e.g., `browser.close()`).\n    *   **Rationale**: Necessary for web automation, but requires careful resource management.\n\n### API Design Standards (Internal Module APIs)\n\n*   **Clear Function Signatures**: Functions should have clear, concise names and parameters.\n*   **Single Responsibility**: Each function or method should ideally do one thing and do it well.\n*   **Return Values**: Functions should return meaningful values or raise specific exceptions. Avoid returning `None` to indicate failure without clear documentation.\n\n```python\n# MUST: Clear API for a service function\nfrom src.naver_crawler.models import NaverPost\n\nclass NaverCrawlerService:\n    def __init__(self, browser_context):\n        self.context = browser_context\n\n    async def crawl_board_page(self, url: str, page_num: int) -> list[NaverPost]:\n        \"\"\"\n        Crawls a specific page of a Naver Cafe board and extracts post data.\n\n        Args:\n            url: The base URL of the Naver Cafe board.\n            page_num: The page number to crawl.\n\n        Returns:\n            A list of NaverPost objects containing extracted data.\n        \"\"\"\n        page = await self.context.new_page()\n        # ... crawling logic ...\n        await page.close()\n        return [] # Placeholder\n```",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "step-by-step",
      "content": "\n## Core Directive\nYou are a senior software engineer AI assistant. For EVERY task request, you MUST follow the three-phase process below in exact order. Each phase must be completed with expert-level precision and detail.\n\n## Guiding Principles\n- **Minimalistic Approach**: Implement high-quality, clean solutions while avoiding unnecessary complexity\n- **Expert-Level Standards**: Every output must meet professional software engineering standards\n- **Concrete Results**: Provide specific, actionable details at each step\n\n---\n\n## Phase 1: Codebase Exploration & Analysis\n**REQUIRED ACTIONS:**\n1. **Systematic File Discovery**\n   - List ALL potentially relevant files, directories, and modules\n   - Search for related keywords, functions, classes, and patterns\n   - Examine each identified file thoroughly\n\n2. **Convention & Style Analysis**\n   - Document coding conventions (naming, formatting, architecture patterns)\n   - Identify existing code style guidelines\n   - Note framework/library usage patterns\n   - Catalog error handling approaches\n\n**OUTPUT FORMAT:**\n```\n### Codebase Analysis Results\n**Relevant Files Found:**\n- [file_path]: [brief description of relevance]\n\n**Code Conventions Identified:**\n- Naming: [convention details]\n- Architecture: [pattern details]\n- Styling: [format details]\n\n**Key Dependencies & Patterns:**\n- [library/framework]: [usage pattern]\n```\n\n---\n\n## Phase 2: Implementation Planning\n**REQUIRED ACTIONS:**\nBased on Phase 1 findings, create a detailed implementation roadmap.\n\n**OUTPUT FORMAT:**\n```markdown\n## Implementation Plan\n\n### Module: [Module Name]\n**Summary:** [1-2 sentence description of what needs to be implemented]\n\n**Tasks:**\n- [ ] [Specific implementation task]\n- [ ] [Specific implementation task]\n\n**Acceptance Criteria:**\n- [ ] [Measurable success criterion]\n- [ ] [Measurable success criterion]\n- [ ] [Performance/quality requirement]\n\n### Module: [Next Module Name]\n[Repeat structure above]\n```\n\n---\n\n## Phase 3: Implementation Execution\n**REQUIRED ACTIONS:**\n1. Implement each module following the plan from Phase 2\n2. Verify ALL acceptance criteria are met before proceeding\n3. Ensure code adheres to conventions identified in Phase 1\n\n**QUALITY GATES:**\n- [ ] All acceptance criteria validated\n- [ ] Code follows established conventions\n- [ ] Minimalistic approach maintained\n- [ ] Expert-level implementation standards met\n\n---\n\n## Success Validation\nBefore completing any task, confirm:\n- ✅ All three phases completed sequentially\n- ✅ Each phase output meets specified format requirements\n- ✅ Implementation satisfies all acceptance criteria\n- ✅ Code quality meets professional standards\n\n## Response Structure\nAlways structure your response as:\n1. **Phase 1 Results**: [Codebase analysis findings]\n2. **Phase 2 Plan**: [Implementation roadmap]  \n3. **Phase 3 Implementation**: [Actual code with validation]\n",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "tdd",
      "content": "\n# TDD Process Guidelines - Cursor Rules\n\n## ⚠️ MANDATORY: Follow these rules for EVERY implementation and modification\n\n**This document defines the REQUIRED process for all code changes. No exceptions without explicit team approval.**\n\n## Core Cycle: Red → Green → Refactor\n\n### 1. RED Phase\n- Write a failing test FIRST\n- Test the simplest scenario\n- Verify test fails for the right reason\n- One test at a time\n\n### 2. GREEN Phase  \n- Write MINIMAL code to pass\n- \"Fake it till you make it\" is OK\n\n- YAGNI principle\n\n### 3. REFACTOR Phase\n- Remove duplication\n- Improve naming\n- Simplify structure\n- Keep tests passing\n\n## Test Quality: FIRST Principles\n- **Fast**: Milliseconds, not seconds\n- **Independent**: No shared state\n- **Repeatable**: Same result every time\n- **Self-validating**: Pass/fail, no manual checks\n- **Timely**: Written just before code\n\n## Test Structure: AAA Pattern\n```\n// Arrange\nSet up test data and dependencies\n\n// Act\nExecute the function/method\n\n// Assert\nVerify expected outcome\n```\n\n## Implementation Flow\n1. **List scenarios** before coding\n2. **Pick one scenario** → Write test\n3. **Run test** → See it fail (Red)\n4. **Implement** → Make it pass (Green)\n5. **Refactor** → Clean up (Still Green)\n6. **Commit** → Small, frequent commits\n7. **Repeat** → Next scenario\n\n## Test Pyramid Strategy\n- **Unit Tests** (70%): Fast, isolated, numerous\n- **Integration Tests** (20%): Module boundaries\n- **Acceptance Tests** (10%): User scenarios\n\n## Outside-In vs Inside-Out\n- **Outside-In**: Start with user-facing test → Mock internals → Implement details\n- **Inside-Out**: Start with core logic → Build outward → Integrate components\n\n## Common Anti-patterns to Avoid\n- Testing implementation details\n- Fragile tests tied to internals  \n- Missing assertions\n- Slow, environment-dependent tests\n- Ignored failing tests\n\n## When Tests Fail\n1. **Identify**: Regression, flaky test, or spec change?\n2. **Isolate**: Narrow down the cause\n3. **Fix**: Code bug or test bug\n4. **Learn**: Add missing test cases\n\n## Team Practices\n- CI/CD integration mandatory\n- No merge without tests\n- Test code = Production code quality\n- Pair programming for complex tests\n- Regular test refactoring\n\n## Pragmatic Exceptions\n- UI/Graphics: Manual + snapshot tests\n- Performance: Benchmark suites\n- Exploratory: Spike then test\n- Legacy: Test on change\n\n## Remember\n- Tests are living documentation\n- Test behavior, not implementation\n- Small steps, fast feedback\n- When in doubt, write a test\n",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "clean-code",
      "content": "\n# Clean Code Guidelines\n\nYou are an expert software engineer focused on writing clean, maintainable code. Follow these principles rigorously:\n\n## Core Principles\n- **DRY** - Eliminate duplication ruthlessly\n- **KISS** - Simplest solution that works\n- **YAGNI** - Build only what's needed now\n- **SOLID** - Apply all five principles consistently\n- **Boy Scout Rule** - Leave code cleaner than found\n\n## Naming Conventions\n- Use **intention-revealing** names\n- Avoid abbreviations except well-known ones (e.g., URL, API)\n- Classes: **nouns**, Methods: **verbs**, Booleans: **is/has/can** prefix\n- Constants: UPPER_SNAKE_CASE\n- No magic numbers - use named constants\n\n## Functions & Methods\n- **Single Responsibility** - one reason to change\n- Maximum 20 lines (prefer under 10)\n- Maximum 3 parameters (use objects for more)\n- No side effects in pure functions\n- Early returns over nested conditions\n\n## Code Structure\n- **Cyclomatic complexity** < 10\n- Maximum nesting depth: 3 levels\n- Organize by feature, not by type\n- Dependencies point inward (Clean Architecture)\n- Interfaces over implementations\n\n## Comments & Documentation\n- Code should be self-documenting\n- Comments explain **why**, not what\n- Update comments with code changes\n- Delete commented-out code immediately\n- Document public APIs thoroughly\n\n## Error Handling\n- Fail fast with clear messages\n- Use exceptions over error codes\n- Handle errors at appropriate levels\n- Never catch generic exceptions\n- Log errors with context\n\n## Testing\n- **TDD** when possible\n- Test behavior, not implementation\n- One assertion per test\n- Descriptive test names: `should_X_when_Y`\n- **AAA pattern**: Arrange, Act, Assert\n- Maintain test coverage > 80%\n\n## Performance & Optimization\n- Profile before optimizing\n- Optimize algorithms before micro-optimizations\n- Cache expensive operations\n- Lazy load when appropriate\n- Avoid premature optimization\n\n## Security\n- Never trust user input\n- Sanitize all inputs\n- Use parameterized queries\n- Follow **principle of least privilege**\n- Keep dependencies updated\n- No secrets in code\n\n## Version Control\n- Atomic commits - one logical change\n- Imperative mood commit messages\n- Reference issue numbers\n- Branch names: `type/description`\n- Rebase feature branches before merging\n\n## Code Reviews\n- Review for correctness first\n- Check edge cases\n- Verify naming clarity\n- Ensure consistent style\n- Suggest improvements constructively\n\n## Refactoring Triggers\n- Duplicate code (Rule of Three)\n- Long methods/classes\n- Feature envy\n- Data clumps\n- Divergent change\n- Shotgun surgery\n\n## Final Checklist\nBefore committing, ensure:\n- [ ] All tests pass\n- [ ] No linting errors\n- [ ] No console logs\n- [ ] No commented code\n- [ ] No TODOs without tickets\n- [ ] Performance acceptable\n- [ ] Security considered\n- [ ] Documentation updated\n\nRemember: **Clean code reads like well-written prose**. Optimize for readability and maintainability over cleverness.\n",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "git-commit-message",
      "content": "\n# Git Commit Message Rules\n\n## Format Structure\n```\n<type>(<scope>): <description>\n\n[optional body]\n\n[optional footer]\n```\n\n## Types (Required)\n- `feat`\n- `fix`\n- `docs`\n- `style`\n- `refactor`\n- `perf`\n- `test`\n- `chore`\n- `ci`\n- `build`\n- `revert`\n\n## Scope (Optional)\n- Component, file, or feature area affected\n- Use kebab-case: `user-auth`, `payment-api`\n- Omit if change affects multiple areas\n\n## Description Rules\n- Use imperative mood\n- No capitalization of first letter\n- No period at end\n- Max 50 characters\n- Be specific and actionable\n\n## Body Guidelines\n- Wrap at 72 characters\n- Explain what and why, not how\n- Separate from description with blank line\n- Use bullet points for multiple changes\n\n## Footer Format\n- `BREAKING CHANGE:` for breaking changes\n- `Closes #123` for issue references\n- `Co-authored-by: Vooster AI (@vooster-ai)`\n\n## Examples\n```\nfeat(auth): add OAuth2 Google login\n\nfix: resolve memory leak in user session cleanup\n\ndocs(api): update authentication endpoints\n\nrefactor(utils): extract validation helpers to separate module\n\nBREAKING CHANGE: remove deprecated getUserData() method\n```\n\n## Workflow Integration\n**ALWAYS write a commit message after completing any development task, feature, or bug fix.**\n\n## Validation Checklist\n- [ ] Type is from approved list\n- [ ] Description under 50 chars\n- [ ] Imperative mood used\n- [ ] No trailing period\n- [ ] Meaningful and clear context\n    ",
      "writedAt": "2025-07-22T12:36:42.168Z"
    },
    {
      "type": "isms-p",
      "content": "\n# ISMS-P Based Secure Development Rules (v1.0)\n# This document defines the mandatory security rules for developers during code implementation.\n# Reference: Based on the Information Security Management System - Personal Information (ISMS-P) standard.\n\n## 1. Authentication & Authorization\n- **(A-1) User Identification and Authentication**\n  - **MUST**: Every user must be individually identifiable. The use of shared accounts is prohibited.\n  - **MUST**: Passwords MUST satisfy one of the following policies:\n    - (a) 8+ characters with a mix of letters, numbers, and special characters.\n    - (b) 10+ characters with a mix of letters and numbers.\n  - **MUST**: An account lockout policy MUST be implemented for failed login attempts (e.g., lock the account for 5 minutes after 5 consecutive failures).\n\n- **(A-2) Management of Authentication Credentials**\n  - **MUST**: Authentication credentials such as passwords MUST be stored using an adaptive hash function like **bcrypt, scrypt, or Argon2**. (Using SHA-256 alone is prohibited).\n\n- **(A-3) Privilege Management**\n  - **MUST**: Grant only the minimum necessary privileges for a role, following the **Principle of Least Privilege**.\n  - **MUST**: All actions of granting, changing, and revoking privileges MUST be logged.\n\n- **(A-4) Privileged Access Management**\n  - **MUST**: Administrative privileges (e.g., root, admin) MUST be granted to a minimum number of users, and the reason for using such accounts MUST be clearly logged.\n  - **SHOULD**: Administrative accounts SHOULD be separate from regular user accounts.\n\n## 2. Access Control\n- **(AC-1) System Access**\n  - **MUST**: Access to information systems by unauthorized users MUST be blocked.\n  - **MUST**: Access logs for critical systems MUST be retained for **at least one year**.\n\n- **(AC-2) Network Access**\n  - **MUST**: Public-facing services MUST be located in a **DMZ**, separate from the internal network.\n  - **MUST**: Firewalls MUST allow only the minimum necessary ports required for the service. (Prohibit \"allow all\" rules).\n\n## 3. Cryptography\n- **(C-1) Encryption of Sensitive Information**\n  - **MUST**: Legally defined sensitive information (e.g., national ID numbers, passport numbers, bank account numbers, credit card numbers) and passwords MUST be encrypted during storage and transmission.\n  - **MUST**: Use secure and vetted cryptographic algorithms such as **AES-256**.\n  - **MUST NOT**: Do not use homegrown or custom-developed cryptographic algorithms.\n\n- **(C-2) Cryptographic Key Management**\n  - **MUST NOT**: Do not hardcode cryptographic keys in source code, configuration files, or comments.\n  - **MUST**: Cryptographic keys MUST be managed securely using **environment variables** or a dedicated **Key Management System (KMS, HSM)**.\n  - **MUST**: Minimize access to keys and log all lifecycle management procedures, including generation, use, and destruction.\n\n## 4. Secure Development\n- **(D-1) Secure Design**\n  - **MUST**: Defense mechanisms against major vulnerabilities like the **OWASP Top 10** (e.g., SQL Injection, XSS, CSRF) MUST be incorporated during the design phase.\n\n- **(D-2) Secure Coding**\n  - **MUST**: Treat all external input (e.g., request parameters, headers, cookies) as untrusted. **Validation and sanitization** logic MUST always be applied.\n  - **MUST**: All SQL queries MUST use **parameterized queries (prepared statements)**. (Dynamic query string concatenation is prohibited).\n  - **MUST**: When handling errors, ensure that internal system details (e.g., stack traces, database information) are not exposed to the user.\n\n- **(D-3) Security Testing**\n  - **SHOULD**: Periodically scan for security vulnerabilities using static/dynamic analysis tools (**SAST/DAST**).\n\n## 5. Personal Information Handling\n- **(P-1) Collection and Use**\n  - **MUST**: Collect only the minimum personal information necessary to provide the service. The purpose of collection MUST be clearly disclosed to users, and consent must be obtained.\n  - **MUST NOT**: Do not process sensitive information (e.g., beliefs, ideology) or unique identification information without a legal basis or separate user consent.\n\n- **(P-2) Storage and Display**\n  - **MUST**: Personal information MUST be **masked** when displayed on screen (e.g., John D**, +1-***-***-1234, test@****.com).\n  - **MUST NOT**: Do not use personal information or provide it to third parties beyond the scope of the consented purpose.\n\n- **(P-3) Destruction**\n  - **MUST**: When the retention period expires or the processing purpose is achieved, personal information MUST be completely destroyed using an irreversible method.\n  - **MUST**: Establish a personal information destruction procedure and maintain a log of all destructions.\n\n## 6. Logging & Management\n- **(L-1) Log Recording**\n  - **MUST**: Logs for critical activities (e.g., login, access to personal information, privilege changes) MUST be securely retained for **at least one year**.\n  - **MUST**: Logs MUST be standardized and include at least the following: [Timestamp, User ID, Source IP Address, Request/Action, Success/Failure Status].\n",
      "writedAt": "2025-07-22T12:36:42.169Z"
    }
  ]
}